---
defaults:
  - &setup-gcloud
    name: Setup Google Cloud SDK
    command: |
      echo $GCLOUD_SERVICE_KEY > ${HOME}/gcloud-service-key.json
      gcloud auth activate-service-account --key-file=${HOME}/gcloud-service-key.json
      gcloud --quiet config set project ${GOOGLE_PROJECT_ID}
      gcloud --quiet config set compute/zone ${GOOGLE_COMPUTE_ZONE}
      gcloud --quiet container clusters get-credentials ${GOOGLE_CLUSTER_NAME}
  - &reveal-git-secret
    name: Reveal git secrets
    command: |
      echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
      gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
      git secret reveal
  - &gcloud-env
    PROJECT_NAME: "CPDB"
    GOOGLE_PROJECT_ID: "twitterbot-180604"
    GOOGLE_COMPUTE_ZONE: "us-central1-a"
    GOOGLE_CLUSTER_NAME: "cpdp-gke"
version: 2
jobs:
  test:
    docker:
      - image: cpdbdev/backend:py3
        user: gunicorn
      - image: cpdbdev/postgres:9.6
      - image: elasticsearch:5-alpine
        name: elasticsearch
      - image: selenium/standalone-chrome:3.11
    environment:
      DJANGO_SETTINGS_MODULE: config.settings.circleci
      DB_HOST: localhost
      DB_USER: cpdb
      DB_PASSWORD: password
      DB_NAME: cpdb
    steps:
      - run: echo 'export PATH="/home/gunicorn/.local/bin/:$PATH"' >> $BASH_ENV
      - checkout
      - run: pip install --user -r requirements/test.txt
      - run: flake8
      - run: ./cpdb/manage.py makemigrations --dry-run --settings=config.settings.test
      - run: coverage run cpdb/manage.py test --noinput --nologcapture
      - run: coverage report --omit="/home/ubuntu/virtualenvs/*"
      - run:
          name: Submit coverage
          command: |
            if [ -z $SKIP_COVERALLS ] || [ $SKIP_COVERALLS != "true" ]; then
              coveralls
            else
              echo "Skipped due to SKIP_COVERALLS is" $SKIP_COVERALLS
            fi
      - store_artifacts:
          path: /usr/src/app/project/cpdb/test_visual_token_media

  test_lambda:
    docker:
      - image: cpdbdev/backend:py3
        user: gunicorn
    environment:
      DJANGO_SETTINGS_MODULE: config.settings.circleci
    steps:
      - run: echo 'export PATH="/home/gunicorn/.local/bin/:$PATH"' >> $BASH_ENV
      - checkout
      - run: pip install --user -r requirements/test.txt
      - run: python -m lambda.test

  push_backend_image:
    machine: true
    steps:
      - checkout
      - run: echo "build-$CIRCLE_BUILD_NUM" > buildnum
      - persist_to_workspace:
          root: .
          paths:
            - buildnum
      - restore_cache:
          keys:
            - v3-{{ checksum "Dockerfile" }}-{{ checksum "requirements/base.txt" }}-{{ .Branch }}
            - v3-{{ checksum "Dockerfile" }}-{{ checksum "requirements/base.txt" }}
            - v3-{{ checksum "Dockerfile" }}
            - v3
          paths:
            - /home/circleci/caches/app.tar
      - run:
          name: Load Docker image layer cache
          command: |
            set +o pipefail
            docker load -i /home/circleci/caches/app.tar | true
          no_output_timeout: 20m
      - run:
          name: Build image
          command: |
            docker build -t cpdbdev/backend:$(cat buildnum) .
      - run:
          name: Push image
          command: |
            echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin
            docker push cpdbdev/backend:$(cat buildnum)
      - run:
          name: Save Docker image layer cache
          command: |
            mkdir -p /home/circleci/caches
            docker save -o /home/circleci/caches/app.tar $(docker images -a -q | tr '\r\n' ' ')
          no_output_timeout: 30m
      - save_cache:
          key: v3-{{ checksum "Dockerfile" }}-{{ checksum "requirements/base.txt" }}-{{ .Branch }}
          paths:
            - /home/circleci/caches/app.tar

  django_collect_static:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *setup-gcloud
      - run: *reveal-git-secret
      - run:
          name: Start job
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export ENVFLAG=--production
            elif [ $CIRCLE_BRANCH == "beta" ]
            then
              export ENVFLAG=--beta
            else
              export ENVFLAG=--staging
            fi
            bin/run_job.sh $ENVFLAG $(cat /tmp/workspace/buildnum) collectstatic --no-input

  django_migrate:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *setup-gcloud
      - run: *reveal-git-secret
      - run:
          name: Start job
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export ENVFLAG=--production
            elif [ $CIRCLE_BRANCH == "beta" ]
            then
              export ENVFLAG=--beta
            else
              export ENVFLAG=--staging
            fi
            bin/run_job.sh $ENVFLAG $(cat /tmp/workspace/buildnum) migrate

  clear_cache:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *setup-gcloud
      - run: *reveal-git-secret
      - run:
          name: Start job
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export ENVFLAG=--production
            elif [ $CIRCLE_BRANCH == "beta" ]
            then
              export ENVFLAG=--beta
            else
              export ENVFLAG=--staging
            fi
            bin/run_job.sh $ENVFLAG $(cat /tmp/workspace/buildnum) clear_cache

  rebuild_index:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *reveal-git-secret
      - run: *setup-gcloud
      - run:
          name: Start job
          no_output_timeout: 4h
          command: |
            if [ -z $SKIP_REBUILD_INDEX ] || [ $SKIP_REBUILD_INDEX != "true" ]; then
              if [ $CIRCLE_BRANCH == "master" ]
              then
                export ENV_TAG=--production
              elif [ $CIRCLE_BRANCH == "beta" ]
              then
                export ENV_TAG=--beta
              else
                export ENV_TAG=--staging
              fi
              bin/run_job.sh $ENV_TAG $(cat /tmp/workspace/buildnum) rebuild_index
            else
              echo "Skipped due to $SKIP_REBUILD_INDEX is" $SKIP_REBUILD_INDEX
            fi

  rebuild_search_index:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *reveal-git-secret
      - run: *setup-gcloud
      - run:
          name: Start job
          no_output_timeout: 4h
          command: |
            if [ -z $SKIP_REBUILD_SEARCH_INDEX ] || [ $SKIP_REBUILD_SEARCH_INDEX != "true" ]; then
              if [ $CIRCLE_BRANCH == "master" ]
              then
                export ENV_TAG=--production
                elif [ $CIRCLE_BRANCH == "beta" ]
              then
                export ENV_TAG=--beta
              else
                export ENV_TAG=--staging
              fi
              bin/run_job.sh $ENV_TAG $(cat /tmp/workspace/buildnum) rebuild_search_index
            else
              echo "Skipped due to SKIP_REBUILD_SEARCH_INDEX is" $SKIP_REBUILD_SEARCH_INDEX
            fi

  deploy_backend:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *reveal-git-secret
      - run: *setup-gcloud
      - run:
          name: Apply deployment
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export NAMESPACE=production
              export ENV_FILE=prod.env
            elif [ $CIRCLE_BRANCH == "beta" ]
            then
              export NAMESPACE=beta
              export ENV_FILE=beta.env
            else
              export NAMESPACE=staging
              export ENV_FILE=staging.env
            fi
            export BACKEND_IMAGE_TAG=$(cat /tmp/workspace/buildnum)
            source $ENV_FILE
            export $(cut -d= -f1 $ENV_FILE)
            bin/ensure_gunicorn_deployment.py -n $NAMESPACE
      - run:
          name: Setup cronjobs
          command: |
            kept_crobjobs=('update_documents', 'crawl_copa_data', 'upload_document_requests', 'update_attachment_downloads_and_views_count', 'update_titles_to_documentcloud', 'rebuild_index_daily', 'webhook_statuses_check')
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export ENV_TAG="--production"
              REBUILD_INDEX_TIME=7
            elif [ $CIRCLE_BRANCH == "beta" ]
            then
              export ENV_TAG="--beta"
              REBUILD_INDEX_TIME=9
            else
              export ENV_TAG="--staging"
              REBUILD_INDEX_TIME=11
            fi
            bin/cleanup_cronjobs.sh $ENV_TAG "$kept_crobjobs"
            bin/run_cronjob.sh $ENV_TAG update_documents "0 5 * * *" $(cat /tmp/workspace/buildnum)
            bin/run_cronjob.sh $ENV_TAG crawl_copa_data "0 5 * * *" $(cat /tmp/workspace/buildnum)
            bin/run_cronjob.sh $ENV_TAG upload_document_requests @hourly $(cat /tmp/workspace/buildnum)
            bin/run_cronjob.sh $ENV_TAG update_attachment_downloads_and_views_count "0 5 * * *" $(cat /tmp/workspace/buildnum)
            bin/run_cronjob.sh $ENV_TAG update_titles_to_documentcloud "0 5 * * *" $(cat /tmp/workspace/buildnum)
            bin/run_cronjob.sh $ENV_TAG rebuild_index_daily "0 $REBUILD_INDEX_TIME * * *" $(cat /tmp/workspace/buildnum)
            if [ $CIRCLE_BRANCH == "master" ]
            then
              bin/run_cronjob.sh $ENV_TAG webhook_statuses_check @daily $(cat /tmp/workspace/buildnum)
            fi

  build_cpdpbot:
    machine: true
    steps:
      - checkout
      - run: echo "build-$CIRCLE_BUILD_NUM" > buildnum
      - persist_to_workspace:
          root: .
          paths:
            - buildnum
      - run:
          name: Build cpdpbot image
          command: docker build -t cpdbdev/cpdpbot:$(cat buildnum) docker/cpdpbot
      - run:
          name: Test cpdpbot image
          command: docker run -e "SETUP_LOGGING=no" --rm cpdbdev/cpdpbot:$(cat buildnum) python -m cpdpbot.test
      - run:
          name: Push cpdpbot image
          command: |
            echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin
            docker push cpdbdev/cpdpbot:$(cat buildnum)

  deploy_cpdpbot:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment: *gcloud-env
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: *setup-gcloud
      - run: *reveal-git-secret
      - run:
          name: Deploy cpdpbot
          command: |
            export CPDPBOT_IMAGE_TAG=$(cat /tmp/workspace/buildnum)
            source prod.env
            export $(cut -d= -f1 prod.env)
            cat kubernetes/cpdpbot.yml | envsubst | kubectl apply --namespace=production -f -

  notify_slack:
    docker:
      - image: python:3.7-alpine
    steps:
      - run: apk add git
      - run: pip install requests
      - checkout
      - run: bin/notify_slack.py

  reset_staging_db:
    docker:
      - image: cpdbdev/google-cloud-sdk:latest
    environment:
      <<: *gcloud-env
      PROD_DB_INSTANCE: "cpdp-production"
      STAGING_DB_INSTANCE: "cpdp"
    steps:
      - run: *setup-gcloud
      - run: gcloud sql backups create --instance $PROD_DB_INSTANCE
      - run:
          name: Restore production backup onto staging
          command: |
            export BACKUP_ID=$(gcloud sql backups list --instance $PROD_DB_INSTANCE --limit 1 --uri | sed -En 's/.+\/([0-9]+)$/\1/p')
            echo "Start restoring backup $BACKUP_ID onto staging"
            gcloud sql backups restore $BACKUP_ID --restore-instance=$STAGING_DB_INSTANCE --backup-instance=$PROD_DB_INSTANCE --quiet --async

  reset_staging_branch:
    docker:
      - image: alpine/git:latest
    steps:
      - checkout
      - run: git checkout staging
      - run: git reset --hard origin/develop
      - run: git push --force origin staging

workflows:
  version: 2
  test:
    jobs:
      - test:
          filters:
            branches:
              ignore:
                - staging
                - beta
                - master

  deploy_backend_staging:
    jobs:
      - test:
          filters:
            branches:
              only: staging
      - push_backend_image:
          requires:
            - test
      - django_collect_static:
          requires:
            - push_backend_image
      - django_migrate:
          requires:
            - push_backend_image
      - deploy_backend:
          requires:
            - django_migrate
            - django_collect_static
      - clear_cache:
          requires:
            - deploy_backend

  deploy_backend_beta:
    jobs:
      - test:
          filters:
            branches:
              only: beta
      - push_backend_image:
          requires:
            - test
      - django_collect_static:
          requires:
            - push_backend_image
      - django_migrate:
          requires:
            - push_backend_image
      - rebuild_index:
          requires:
            - django_migrate
      - rebuild_search_index:
          requires:
            - django_migrate
      - deploy_backend:
          requires:
            - rebuild_index
            - rebuild_search_index
            - django_collect_static
      - clear_cache:
          requires:
            - deploy_backend

  deploy_backend_production:
    jobs:
      - test:
          filters:
            branches:
              only: master
      - push_backend_image:
          requires:
            - test
      - django_collect_static:
          requires:
            - push_backend_image
      - django_migrate:
          requires:
            - push_backend_image
      - rebuild_index:
          requires:
            - django_migrate
      - rebuild_search_index:
          requires:
            - django_migrate
      - deploy_backend:
          requires:
            - rebuild_index
            - rebuild_search_index
            - django_collect_static
      - clear_cache:
          requires:
            - deploy_backend
      - notify_slack:
          requires:
            - deploy_backend

  deploy_cpdpbot:
    jobs:
      - build_cpdpbot:
          filters:
            branches:
              only:
                - staging
                - master
      - deploy_cpdpbot:
          requires:
            - build_cpdpbot
          filters:
            branches:
              only:
                - master
